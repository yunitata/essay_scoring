{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Automatic Short Answer Scoring menggunakan Python \n",
    "Ada beberapa tahapan yang diperlukan dalam pengembangan sistem, yaitu:\n",
    "1. Preparation\n",
    "2. Pre-processing\n",
    "3. Ekstraksi Fitur\n",
    "4. Learning\n",
    "\n",
    "### 1. Preparation\n",
    "Dalam tahap preparation, kita perlu mempersiapkan reference answers dan kamus *stopwords* terlebih dahulu.\n",
    "* Stopwords merupakan kata-kata yang biasanya tidak signifikan secara makna (non-content words) (i.e. yang, dari, ke, dia, kamu, etc). Kita bisa juga menambahkan dalam list *stopwords* , kata-kata yang tidak baku (yg, karna). Stopwords di list dalam file **stopwords.txt**\n",
    "* Reference answer diambil berdasarkan petunjuk soal. \n",
    "Selain itu data essay (Maccu Pichu/Jaket) juga di-import.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biro Perjalanan Buana Wisata menawarkan berbagai perjalanan wisata yang akan membantu mengubah kota batu yang sunyi ini menjadi hidup.Biro perjalanan buana wisata menawarkan akses ketempat - tempat yang jarang diketahui wisatawan.\n",
      "loading data\n",
      "/home/yunita/Data/Project/essay_scoring/Dataset/macu_picu/maccu-piccu_rescore.xlsx\n",
      "6618\n",
      "6618\n",
      "set([0, 1])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"/home/yunita/Data/Project/essay_scoring/Dataset/macu_picu/maccu-piccu_rescore.xlsx\"\n",
    "df = pd.read_excel(filename, sheetname='Sheet1', header=0)\n",
    "df_ = df[df['rescore_ori'] != 9]  # membuang row yang kosong\n",
    "ans1 = np.array(df_['fte_data1'])\n",
    "ans2 = np.array(df_['fte_data2'])\n",
    "label = np.array(df_['rescore_ori'])\n",
    "\n",
    "print ans1[0]\n",
    "\n",
    "print (\"loading data\")\n",
    "print filename\n",
    "print len(ans1)\n",
    "print len(ans2)\n",
    "print set(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Memanggil kamus *stopwords* yang terdiri dari 765 kata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765\n"
     ]
    }
   ],
   "source": [
    "fstopw = open('/home/yunita/Data/Code/essay_scoring/stopwords.txt','r')\n",
    "stopw = set([w for w in fstopw])\n",
    "stopadd = ['karena','dia','adalah','jadi','yg','dan','ia','ini']\n",
    "for s in stopadd:\n",
    "  stopw.add(s)\n",
    "print (len(stopw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-processing\n",
    "Setelah data di-import dan kamus stopword sudah dipersiapkan, maka tahap berikutnya adalah melakukan pre-processing dengan membuang semua *stopwords* dan menyeragamkan jawaban dalam huruf kecil (*lowercasing*), untuk meminimalisir perbedaan penulisan jawaban.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer = []\n",
    "for i in range(len(label)):\n",
    "    ans = []\n",
    "    if ans1[i] is np.nan:\n",
    "        ans = ans2[i]\n",
    "    elif ans2[i] is np.nan:\n",
    "        ans = ans1[i]\n",
    "    else:\n",
    "        ans = ans1[i]+ans2[i]\n",
    "    ans_lower = ans.lower()\n",
    "    ans_stop = [word for word in ans_lower.split() if word not in stopw]\n",
    "    answer.append(' '.join(ans_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ekstraksi Fitur\n",
    "Tahap berikutnya adalah ekstraksi fitur. Untuk eksperimen kali ini, kita akan menggunakan frekuensi kata yang terdapat pada jawaban sebagai fitur. Untuk ekstraksi fitur, digunakan salah satu python library yang disebut Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer=\"word\")\n",
    "train_data = count_vect.fit_transform(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Learning\n",
    "Tahap selanjutnya adalah learning dengan mengimplementasikan algoritma machine learning. Kita bisa mencoba beberapa algoritma dengan *cross-validation* untuk melihat algoritma mana yang paling optimal. Pada experiment dengan menggunakan 5-cross validation, dengan 4 metode yang berbeda, diperoleh akurasi tertinggi yaitu 97.26% dengan menggunakan Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Naive Bayes:', 0.9468109986867814)\n",
      "('SVM:', 0.9530045832744669)\n",
      "('Random Forest:', 0.6199748863703392)\n",
      "('Ada Boost:', 0.972648557616058)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "clf1 = MultinomialNB()\n",
    "clf2 = svm.SVC(probability=True, random_state=0)\n",
    "clf3 = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "clf4 = AdaBoostClassifier()\n",
    "\n",
    "print (\"Naive Bayes:\", np.mean(cross_val_score(clf1, train_data, label, scoring='accuracy',cv=5))) \n",
    "print (\"SVM:\", np.mean(cross_val_score(clf2, train_data, label, scoring='accuracy',cv=5))) \n",
    "print (\"Random Forest:\", np.mean(cross_val_score(clf3, train_data, label, scoring='accuracy',cv=5))) \n",
    "print (\"Ada Boost:\", np.mean(cross_val_score(clf4, train_data, label, scoring='accuracy',cv=5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook index.ipynb to html\n",
      "[NbConvertApp] Writing 206047 bytes to index.html\n"
     ]
    }
   ],
   "source": [
    "!ipython nbconvert index.ipynb --to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook index.ipynb to latex\n",
      "[NbConvertApp] ERROR | Error while converting 'index.ipynb'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/nbconvertapp.py\", line 332, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/exporter.py\", line 166, in from_filename\n",
      "    return self.from_notebook_node(nbformat.read(f, as_version=4), resources=resources, **kw)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/latex.py\", line 88, in from_notebook_node\n",
      "    return super(LatexExporter, self).from_notebook_node(nb, resources, **kw)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/templateexporter.py\", line 204, in from_notebook_node\n",
      "    output = self.template.render(nb=nb_copy, resources=resources)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jinja2/environment.py\", line 989, in render\n",
      "    return self.environment.handle_exception(exc_info, True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jinja2/environment.py\", line 754, in handle_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/../templates/latex/article.tplx\", line 8, in top-level template code\n",
      "    ((* extends cell_style *))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/../templates/latex/style_ipython.tplx\", line 56, in top-level template code\n",
      "    ((( text | add_prompts(first='{\\color{' ~ prompt_color ~ '}' ~ prompt ~ '[{\\\\color{' ~ prompt_color ~ '}' ~ execution_count ~ '}]:} ', cont=indention) )))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/../templates/latex/base.tplx\", line 183, in top-level template code\n",
      "    ((*- block figure scoped -*))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/../templates/latex/skeleton/display_priority.tplx\", line 5, in top-level template code\n",
      "    ((*- extends 'null.tplx' -*))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/../templates/latex/skeleton/null.tplx\", line 30, in top-level template code\n",
      "    ((*- block body -*))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/../templates/latex/base.tplx\", line 125, in block \"body\"\n",
      "    ((( super() )))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/../templates/latex/skeleton/null.tplx\", line 32, in block \"body\"\n",
      "    ((*- block any_cell scoped -*))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/../templates/latex/skeleton/null.tplx\", line 76, in block \"any_cell\"\n",
      "    ((*- block markdowncell scoped-*))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/exporters/../templates/latex/base.tplx\", line 200, in block \"markdowncell\"\n",
      "    ((( cell.source | citation2latex | strip_files_prefix | markdown2latex )))\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/filters/markdown.py\", line 59, in markdown2latex\n",
      "    return pandoc(source, markup, 'latex', extra_args=extra_args)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/utils/pandoc.py\", line 49, in pandoc\n",
      "    check_pandoc_version()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/utils/pandoc.py\", line 95, in check_pandoc_version\n",
      "    v = get_pandoc_version()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/utils/pandoc.py\", line 74, in get_pandoc_version\n",
      "    raise PandocMissing()\n",
      "PandocMissing: Pandoc wasn't found.\n",
      "Please check that pandoc is installed:\n",
      "http://johnmacfarlane.net/pandoc/installing.html\n"
     ]
    }
   ],
   "source": [
    "!ipython nbconvert --to latex index.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
