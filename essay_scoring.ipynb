{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Automatic Short Answer Scoring menggunakan Python \n",
    "Ada beberapa tahapan yang diperlukan dalam pengembangan sistem, yaitu:\n",
    "1. Preparation\n",
    "2. Pre-processing\n",
    "3. Ekstraksi Fitur\n",
    "4. Learning\n",
    "\n",
    "### 1. Preparation\n",
    "Dalam tahap preparation, kita perlu mempersiapkan reference answers dan kamus *stopwords* terlebih dahulu.\n",
    "* Stopwords merupakan kata-kata yang biasanya tidak signifikan secara makna (non-content words) (i.e. yang, dari, ke, dia, kamu, etc). Kita bisa juga menambahkan dalam list *stopwords* , kata-kata yang tidak baku (yg, karna). Stopwords di list dalam file **stopwords.txt**\n",
    "* Reference answer diambil berdasarkan petunjuk soal. \n",
    "Selain itu data essay (Maccu Pichu/Jaket) juga di-import.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biro Perjalanan Buana Wisata menawarkan berbagai perjalanan wisata yang akan membantu mengubah kota batu yang sunyi ini menjadi hidup.Biro perjalanan buana wisata menawarkan akses ketempat - tempat yang jarang diketahui wisatawan.\n",
      "loading data\n",
      "/home/yunita/Data/Project/essay_scoring/Dataset/macu_picu/maccu-piccu_rescore.xlsx\n",
      "6618\n",
      "6618\n",
      "set([0, 1])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"/home/yunita/Data/Project/essay_scoring/Dataset/macu_picu/maccu-piccu_rescore.xlsx\"\n",
    "df = pd.read_excel(filename, sheetname='Sheet1', header=0)\n",
    "df_ = df[df['rescore_ori'] != 9]  # membuang row yang kosong\n",
    "ans1 = np.array(df_['fte_data1'])\n",
    "ans2 = np.array(df_['fte_data2'])\n",
    "label = np.array(df_['rescore_ori'])\n",
    "\n",
    "print ans1[0]\n",
    "\n",
    "print (\"loading data\")\n",
    "print filename\n",
    "print len(ans1)\n",
    "print len(ans2)\n",
    "print set(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Memanggil kamus *stopwords* yang terdiri dari 765 kata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765\n"
     ]
    }
   ],
   "source": [
    "fstopw = open('/home/yunita/Data/Code/essay_scoring/stopwords.txt','r')\n",
    "stopw = set([w for w in fstopw])\n",
    "stopadd = ['karena','dia','adalah','jadi','yg','dan','ia','ini']\n",
    "for s in stopadd:\n",
    "  stopw.add(s)\n",
    "print (len(stopw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-processing\n",
    "Setelah data di-import dan kamus stopword sudah dipersiapkan, maka tahap berikutnya adalah melakukan pre-processing dengan membuang semua *stopwords* dan menyeragamkan jawaban dalam huruf kecil (*lowercasing*), untuk meminimalisir perbedaan penulisan jawaban.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answer = []\n",
    "for i in range(len(label)):\n",
    "    ans = []\n",
    "    if ans1[i] is np.nan:\n",
    "        ans = ans2[i]\n",
    "    elif ans2[i] is np.nan:\n",
    "        ans = ans1[i]\n",
    "    else:\n",
    "        ans = ans1[i]+ans2[i]\n",
    "    ans_lower = ans.lower()\n",
    "    ans_stop = [word for word in ans_lower.split() if word not in stopw]\n",
    "    answer.append(' '.join(ans_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ekstraksi Fitur\n",
    "Tahap berikutnya adalah ekstraksi fitur. Untuk eksperimen kali ini, kita akan menggunakan frekuensi kata yang terdapat pada jawaban sebagai fitur. Untuk ekstraksi fitur, digunakan salah satu python library yang disebut Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer=\"word\")\n",
    "train_data = count_vect.fit_transform(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Learning\n",
    "Tahap selanjutnya adalah learning dengan mengimplementasikan algoritma machine learning. Kita bisa mencoba beberapa algoritma dengan *cross-validation* untuk melihat algoritma mana yang paling optimal. Pada experiment dengan menggunakan 5-cross validation, dengan 4 metode yang berbeda, diperoleh akurasi tertinggi yaitu 97.26% dengan menggunakan Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Naive Bayes:', 0.9468109986867814)\n",
      "('SVM:', 0.9530045832744669)\n",
      "('Random Forest:', 0.6199748863703392)\n",
      "('Ada Boost:', 0.972648557616058)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "clf1 = MultinomialNB()\n",
    "clf2 = svm.SVC(probability=True, random_state=0)\n",
    "clf3 = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "clf4 = AdaBoostClassifier()\n",
    "\n",
    "print (\"Naive Bayes:\", np.mean(cross_val_score(clf1, train_data, label, scoring='accuracy',cv=5))) \n",
    "print (\"SVM:\", np.mean(cross_val_score(clf2, train_data, label, scoring='accuracy',cv=5))) \n",
    "print (\"Random Forest:\", np.mean(cross_val_score(clf3, train_data, label, scoring='accuracy',cv=5))) \n",
    "print (\"Ada Boost:\", np.mean(cross_val_score(clf4, train_data, label, scoring='accuracy',cv=5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
